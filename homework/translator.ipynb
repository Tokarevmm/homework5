{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport unicodedata\nimport re\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\n\nimport numpy as np\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:15:47.242518200Z",
     "start_time": "2023-08-25T14:15:47.199604700Z"
    },
    "id": "iSfJVTO5I24F",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:09.595641Z",
     "iopub.execute_input": "2023-08-26T14:57:09.596181Z",
     "iopub.status.idle": "2023-08-26T14:57:09.603922Z",
     "shell.execute_reply.started": "2023-08-26T14:57:09.596077Z",
     "shell.execute_reply": "2023-08-26T14:57:09.602656Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "SOS_token = 0\nEOS_token = 1\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n        self.n_words = 2  # Count SOS and EOS\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:15:47.412980300Z",
     "start_time": "2023-08-25T14:15:47.214449200Z"
    },
    "id": "Z28_vzmmI24J",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:09.606219Z",
     "iopub.execute_input": "2023-08-26T14:57:09.606992Z",
     "iopub.status.idle": "2023-08-26T14:57:09.617944Z",
     "shell.execute_reply.started": "2023-08-26T14:57:09.606956Z",
     "shell.execute_reply": "2023-08-26T14:57:09.616833Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Turn a Unicode string to plain ASCII, thanks to\n# https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n    return s.strip()",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:15:47.414552500Z",
     "start_time": "2023-08-25T14:15:47.214449200Z"
    },
    "id": "9aJbW7E_I24K",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:09.619601Z",
     "iopub.execute_input": "2023-08-26T14:57:09.620330Z",
     "iopub.status.idle": "2023-08-26T14:57:09.632193Z",
     "shell.execute_reply.started": "2023-08-26T14:57:09.620296Z",
     "shell.execute_reply": "2023-08-26T14:57:09.631151Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def readLangs(lang1, lang2, reverse=False):\n    print(\"Reading lines...\")\n\n    # Read the file and split into lines\n    lines = open('/kaggle/input/eng-deu/deu.txt', encoding='utf-8').read().strip().split('\\n')\n\n\n    # Split every line into pairs and normalize\n    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n\n    # Reverse pairs, make Lang instances\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n        input_lang = Lang(lang1)\n        output_lang = Lang(lang2)\n\n    return input_lang, output_lang, pairs",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:24:34.717908100Z",
     "start_time": "2023-08-25T14:24:34.672615400Z"
    },
    "id": "bfuLUCYzI24L",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:09.634993Z",
     "iopub.execute_input": "2023-08-26T14:57:09.635515Z",
     "iopub.status.idle": "2023-08-26T14:57:09.645494Z",
     "shell.execute_reply.started": "2023-08-26T14:57:09.635483Z",
     "shell.execute_reply": "2023-08-26T14:57:09.644434Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "MAX_LENGTH = 10\n\n\ndef filterPair(p):\n    return len(p[0].split(' ')) < MAX_LENGTH and \\\n        len(p[1].split(' ')) < MAX_LENGTH\n\n\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:25:07.437482800Z",
     "start_time": "2023-08-25T14:25:07.390591700Z"
    },
    "id": "_zOuau8HI24N",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:09.647419Z",
     "iopub.execute_input": "2023-08-26T14:57:09.647769Z",
     "iopub.status.idle": "2023-08-26T14:57:09.654591Z",
     "shell.execute_reply.started": "2023-08-26T14:57:09.647736Z",
     "shell.execute_reply": "2023-08-26T14:57:09.653491Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def prepareData(lang1, lang2, reverse=False):\n    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n    print(\"Read %s sentence pairs\" % len(pairs))\n    pairs = filterPairs(pairs)\n    pairs = [[sublist[1], sublist[2]] for sublist in pairs]\n    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n    print(\"Counting words...\")\n    for pair in pairs:\n        input_lang.addSentence(pair[0])\n        output_lang.addSentence(pair[1])\n    print(\"Counted words:\")\n    print(input_lang.name, input_lang.n_words)\n    print(output_lang.name, output_lang.n_words)\n    return input_lang, output_lang, pairs\n\ninput_lang, output_lang, pairs = prepareData('eng', 'deu', True)\nprint(random.choice(pairs))",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:34:44.079951400Z",
     "start_time": "2023-08-25T14:34:20.972417500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4C5q_x8I24N",
    "outputId": "5038c4e2-bebf-4321-f775-7b90ba7c42aa",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:09.655900Z",
     "iopub.execute_input": "2023-08-26T14:57:09.656872Z",
     "iopub.status.idle": "2023-08-26T14:57:37.263196Z",
     "shell.execute_reply.started": "2023-08-26T14:57:09.656839Z",
     "shell.execute_reply": "2023-08-26T14:57:37.261326Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": "Reading lines...\nRead 267186 sentence pairs\nTrimmed to 228101 sentence pairs\nCounting words...\nCounted words:\ndeu 31805\neng 14938\n['wie schreibt man deinen nachnamen ?', 'how do you write your last name ?']\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "print(random.choice(pairs))",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:34:52.105125Z",
     "start_time": "2023-08-25T14:34:52.057706800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkJOeZjKI24R",
    "outputId": "27795b52-85be-471f-a228-b005a42769d1",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:37.264426Z",
     "iopub.execute_input": "2023-08-26T14:57:37.264794Z",
     "iopub.status.idle": "2023-08-26T14:57:37.270639Z",
     "shell.execute_reply.started": "2023-08-26T14:57:37.264760Z",
     "shell.execute_reply": "2023-08-26T14:57:37.269691Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": "['wie feierst du weihnachten ?', 'how do you celebrate christmas ?']\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, input):\n        embedded = self.dropout(self.embedding(input))\n        output, hidden = self.gru(embedded)\n        return output, hidden",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:35:43.439295300Z",
     "start_time": "2023-08-25T14:35:43.356785100Z"
    },
    "id": "_-GpGzHQI24S",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:37.272268Z",
     "iopub.execute_input": "2023-08-26T14:57:37.272952Z",
     "iopub.status.idle": "2023-08-26T14:57:37.284922Z",
     "shell.execute_reply.started": "2023-08-26T14:57:37.272920Z",
     "shell.execute_reply": "2023-08-26T14:57:37.283951Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(DecoderRNN, self).__init__()\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n        self.out = nn.Linear(hidden_size, output_size)\n\n    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n        batch_size = encoder_outputs.size(0)\n        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n        decoder_hidden = encoder_hidden\n        decoder_outputs = []\n\n        for i in range(MAX_LENGTH):\n            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n            decoder_outputs.append(decoder_output)\n\n            if target_tensor is not None:\n                # Teacher forcing: Feed the target as the next input\n                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n            else:\n                # Without teacher forcing: use its own predictions as the next input\n                _, topi = decoder_output.topk(1)\n                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n\n        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n\n    def forward_step(self, input, hidden):\n        output = self.embedding(input)\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n        output = self.out(output)\n        return output, hidden",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:35:50.446409500Z",
     "start_time": "2023-08-25T14:35:50.390190800Z"
    },
    "id": "Kc4pSDxyI24S",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:37.288915Z",
     "iopub.execute_input": "2023-08-26T14:57:37.289206Z",
     "iopub.status.idle": "2023-08-26T14:57:37.301657Z",
     "shell.execute_reply.started": "2023-08-26T14:57:37.289182Z",
     "shell.execute_reply": "2023-08-26T14:57:37.300632Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class BahdanauAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super(BahdanauAttention, self).__init__()\n        self.Wa = nn.Linear(hidden_size, hidden_size)\n        self.Ua = nn.Linear(hidden_size, hidden_size)\n        self.Va = nn.Linear(hidden_size, 1)\n\n    def forward(self, query, keys):\n        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n        scores = scores.squeeze(2).unsqueeze(1)\n\n        weights = F.softmax(scores, dim=-1)\n        context = torch.bmm(weights, keys)\n\n        return context, weights\n\nclass AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n        super(AttnDecoderRNN, self).__init__()\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.attention = BahdanauAttention(hidden_size)\n        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n        batch_size = encoder_outputs.size(0)\n        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n        decoder_hidden = encoder_hidden\n        decoder_outputs = []\n        attentions = []\n\n        for i in range(MAX_LENGTH):\n            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            decoder_outputs.append(decoder_output)\n            attentions.append(attn_weights)\n\n            if target_tensor is not None:\n                # Teacher forcing: Feed the target as the next input\n                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n            else:\n                # Without teacher forcing: use its own predictions as the next input\n                _, topi = decoder_output.topk(1)\n                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n\n        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n        attentions = torch.cat(attentions, dim=1)\n\n        return decoder_outputs, decoder_hidden, attentions\n\n\n    def forward_step(self, input, hidden, encoder_outputs):\n        embedded =  self.dropout(self.embedding(input))\n\n        query = hidden.permute(1, 0, 2)\n        context, attn_weights = self.attention(query, encoder_outputs)\n        input_gru = torch.cat((embedded, context), dim=2)\n\n        output, hidden = self.gru(input_gru, hidden)\n        output = self.out(output)\n\n        return output, hidden, attn_weights",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:36:25.019527700Z",
     "start_time": "2023-08-25T14:36:24.925244800Z"
    },
    "id": "IXhTwG61I24T",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:37.303192Z",
     "iopub.execute_input": "2023-08-26T14:57:37.303638Z",
     "iopub.status.idle": "2023-08-26T14:57:37.323177Z",
     "shell.execute_reply.started": "2023-08-26T14:57:37.303611Z",
     "shell.execute_reply": "2023-08-26T14:57:37.322248Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def indexesFromSentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(' ')]\n\ndef tensorFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(input_lang, pair[0])\n    target_tensor = tensorFromSentence(output_lang, pair[1])\n    return (input_tensor, target_tensor)\n\ndef get_dataloader(batch_size):\n    input_lang, output_lang, pairs = prepareData('eng', 'deu', True)\n\n    n = len(pairs)\n    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n\n    for idx, (inp, tgt) in enumerate(pairs):\n      inp_ids = indexesFromSentence(input_lang, inp)\n      tgt_ids = indexesFromSentence(output_lang, tgt)\n      inp_ids.append(EOS_token)\n      tgt_ids.append(EOS_token)\n\n      inp_ids = inp_ids[:MAX_LENGTH - 1]  # Truncate if needed\n      tgt_ids = tgt_ids[:MAX_LENGTH - 1]  # Truncate if needed\n\n      inp_ids.append(EOS_token)\n      tgt_ids.append(EOS_token)\n\n      input_ids[idx, :len(inp_ids)] = inp_ids\n      target_ids[idx, :len(tgt_ids)] = tgt_ids\n\n\n    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n                               torch.LongTensor(target_ids).to(device))\n\n    train_sampler = RandomSampler(train_data)\n    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n    return input_lang, output_lang, train_dataloader",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:37:00.358113600Z",
     "start_time": "2023-08-25T14:37:00.311626100Z"
    },
    "id": "Q7sEl5ngI24U",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:37.324653Z",
     "iopub.execute_input": "2023-08-26T14:57:37.324988Z",
     "iopub.status.idle": "2023-08-26T14:57:37.338807Z",
     "shell.execute_reply.started": "2023-08-26T14:57:37.324957Z",
     "shell.execute_reply": "2023-08-26T14:57:37.337860Z"
    },
    "trusted": true
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n          decoder_optimizer, criterion):\n\n    total_loss = 0\n    for data in dataloader:\n        input_tensor, target_tensor = data\n\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n        encoder_outputs, encoder_hidden = encoder(input_tensor)\n        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n\n        loss = criterion(\n            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n            target_tensor.view(-1)\n        )\n        loss.backward()\n\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)",
   "metadata": {
    "id": "7YAte41QOx6a",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:37.341014Z",
     "iopub.execute_input": "2023-08-26T14:57:37.341984Z",
     "iopub.status.idle": "2023-08-26T14:57:37.352325Z",
     "shell.execute_reply.started": "2023-08-26T14:57:37.341952Z",
     "shell.execute_reply": "2023-08-26T14:57:37.351401Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import time\nimport math\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:37:15.498742100Z",
     "start_time": "2023-08-25T14:37:15.458199100Z"
    },
    "id": "wrxviFkrI24V",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:37.354781Z",
     "iopub.execute_input": "2023-08-26T14:57:37.355060Z",
     "iopub.status.idle": "2023-08-26T14:57:37.365155Z",
     "shell.execute_reply.started": "2023-08-26T14:57:37.355038Z",
     "shell.execute_reply": "2023-08-26T14:57:37.364133Z"
    },
    "trusted": true
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n               print_every=100, plot_every=100):\n    start = time.time()\n    plot_losses = []\n    print_loss_total = 0  # Reset every print_every\n    plot_loss_total = 0  # Reset every plot_every\n\n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n    criterion = nn.NLLLoss()\n\n    for epoch in range(1, n_epochs + 1):\n        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n        print_loss_total += loss\n        plot_loss_total += loss\n\n        if epoch % print_every == 0:\n            print_loss_avg = print_loss_total / print_every\n            print_loss_total = 0\n            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n\n        if epoch % plot_every == 0:\n            plot_loss_avg = plot_loss_total / plot_every\n            plot_losses.append(plot_loss_avg)\n            plot_loss_total = 0\n\n    showPlot(plot_losses)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:37:22.477320500Z",
     "start_time": "2023-08-25T14:37:22.433915300Z"
    },
    "id": "7aGoFi9yI24V",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:37.367689Z",
     "iopub.execute_input": "2023-08-26T14:57:37.368889Z",
     "iopub.status.idle": "2023-08-26T14:57:37.379705Z",
     "shell.execute_reply.started": "2023-08-26T14:57:37.368864Z",
     "shell.execute_reply": "2023-08-26T14:57:37.378773Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport numpy as np\n\ndef showPlot(points):\n    plt.figure()\n    fig, ax = plt.subplots()\n    # this locator puts ticks at regular intervals\n    loc = ticker.MultipleLocator(base=0.2)\n    ax.yaxis.set_major_locator(loc)\n    plt.plot(points)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:38:00.200486400Z",
     "start_time": "2023-08-25T14:37:57.609706800Z"
    },
    "id": "wVmP96sJI24W",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:37.381202Z",
     "iopub.execute_input": "2023-08-26T14:57:37.381661Z",
     "iopub.status.idle": "2023-08-26T14:57:37.392349Z",
     "shell.execute_reply.started": "2023-08-26T14:57:37.381631Z",
     "shell.execute_reply": "2023-08-26T14:57:37.391347Z"
    },
    "trusted": true
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n    with torch.no_grad():\n        input_tensor = tensorFromSentence(input_lang, sentence)\n\n        encoder_outputs, encoder_hidden = encoder(input_tensor)\n        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n\n        _, topi = decoder_outputs.topk(1)\n        decoded_ids = topi.squeeze()\n\n        decoded_words = []\n        for idx in decoded_ids:\n            if idx.item() == EOS_token:\n                decoded_words.append('<EOS>')\n                break\n            decoded_words.append(output_lang.index2word[idx.item()])\n    return decoded_words, decoder_attn",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:38:06.424368500Z",
     "start_time": "2023-08-25T14:38:06.352087800Z"
    },
    "id": "2ABG2-4FI24W",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:37.393596Z",
     "iopub.execute_input": "2023-08-26T14:57:37.394640Z",
     "iopub.status.idle": "2023-08-26T14:57:37.402661Z",
     "shell.execute_reply.started": "2023-08-26T14:57:37.394606Z",
     "shell.execute_reply": "2023-08-26T14:57:37.401671Z"
    },
    "trusted": true
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('>', pair[0])\n        print('=', pair[1])\n        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        print('')",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:38:15.608240400Z",
     "start_time": "2023-08-25T14:38:15.214199800Z"
    },
    "id": "pc94BX5yI24X",
    "execution": {
     "iopub.status.busy": "2023-08-26T14:57:37.404003Z",
     "iopub.execute_input": "2023-08-26T14:57:37.404490Z",
     "iopub.status.idle": "2023-08-26T14:57:37.415364Z",
     "shell.execute_reply.started": "2023-08-26T14:57:37.404458Z",
     "shell.execute_reply": "2023-08-26T14:57:37.414386Z"
    },
    "trusted": true
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "hidden_size = 128\nbatch_size = 512\n\ninput_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n\nencoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\ndecoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n\ntrain(train_dataloader, encoder, decoder, 200, print_every=5, plot_every=5)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T14:38:35.893532600Z",
     "start_time": "2023-08-25T14:38:22.312397600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99WlIOiII24X",
    "outputId": "d56c6f35-9ff6-4508-d389-b07ce6248178",
    "execution": {
     "iopub.status.busy": "2023-08-26T15:04:59.630288Z",
     "iopub.execute_input": "2023-08-26T15:04:59.630669Z",
     "iopub.status.idle": "2023-08-26T16:36:54.423684Z",
     "shell.execute_reply.started": "2023-08-26T15:04:59.630637Z",
     "shell.execute_reply": "2023-08-26T16:36:54.422205Z"
    },
    "trusted": true
   },
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "text": "Reading lines...\nRead 267186 sentence pairs\nTrimmed to 228101 sentence pairs\nCounting words...\nCounted words:\ndeu 31805\neng 14938\n2m 17s (- 89m 8s) (5 2%) 2.3359\n4m 34s (- 86m 47s) (10 5%) 1.2157\n6m 51s (- 84m 31s) (15 7%) 0.8804\n9m 8s (- 82m 15s) (20 10%) 0.7181\n11m 25s (- 79m 58s) (25 12%) 0.6193\n13m 42s (- 77m 40s) (30 15%) 0.5510\n15m 59s (- 75m 22s) (35 17%) 0.5003\n18m 16s (- 73m 6s) (40 20%) 0.4606\n20m 33s (- 70m 49s) (45 22%) 0.4286\n22m 50s (- 68m 31s) (50 25%) 0.4026\n25m 8s (- 66m 15s) (55 27%) 0.3803\n27m 24s (- 63m 58s) (60 30%) 0.3614\n29m 42s (- 61m 41s) (65 32%) 0.3449\n31m 59s (- 59m 24s) (70 35%) 0.3305\n34m 16s (- 57m 7s) (75 37%) 0.3177\n36m 33s (- 54m 50s) (80 40%) 0.3067\n38m 50s (- 52m 33s) (85 42%) 0.2965\n41m 8s (- 50m 16s) (90 45%) 0.2871\n43m 25s (- 47m 59s) (95 47%) 0.2791\n45m 42s (- 45m 42s) (100 50%) 0.2714\n47m 59s (- 43m 25s) (105 52%) 0.2646\n50m 16s (- 41m 8s) (110 55%) 0.2582\n52m 34s (- 38m 51s) (115 57%) 0.2522\n54m 51s (- 36m 34s) (120 60%) 0.2470\n57m 8s (- 34m 17s) (125 62%) 0.2417\n59m 25s (- 32m 0s) (130 65%) 0.2370\n61m 42s (- 29m 42s) (135 67%) 0.2328\n63m 59s (- 27m 25s) (140 70%) 0.2286\n66m 16s (- 25m 8s) (145 72%) 0.2246\n68m 33s (- 22m 51s) (150 75%) 0.2209\n70m 50s (- 20m 34s) (155 77%) 0.2177\n73m 7s (- 18m 16s) (160 80%) 0.2141\n75m 24s (- 15m 59s) (165 82%) 0.2113\n77m 42s (- 13m 42s) (170 85%) 0.2084\n79m 58s (- 11m 25s) (175 87%) 0.2056\n82m 16s (- 9m 8s) (180 90%) 0.2031\n84m 33s (- 6m 51s) (185 92%) 0.2005\n86m 50s (- 4m 34s) (190 95%) 0.1981\n89m 7s (- 2m 17s) (195 97%) 0.1959\n91m 24s (- 0m 0s) (200 100%) 0.1938\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "encoder.eval()\ndecoder.eval()\nevaluateRandomly(encoder, decoder)",
   "metadata": {
    "id": "Oyo5M6fmI24Y",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "outputId": "ebbcd732-befe-4976-ad41-1cbe50d47de0",
    "execution": {
     "iopub.status.busy": "2023-08-26T16:37:06.080174Z",
     "iopub.execute_input": "2023-08-26T16:37:06.080537Z",
     "iopub.status.idle": "2023-08-26T16:37:06.163504Z",
     "shell.execute_reply.started": "2023-08-26T16:37:06.080507Z",
     "shell.execute_reply": "2023-08-26T16:37:06.162323Z"
    },
    "trusted": true
   },
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "text": "> ich werde von hier verschwinden\n= i m going to get out of here\n< i m going to get out of here from here\n\n> ich schale eine orange fur dich\n= i ll peel an orange for you\n< i ll peel an orange for you <EOS>\n\n> haben sie toms adresse ?\n= do you have tom s address ?\n< do you have tom s address ? <EOS>\n\n> das ist ein bild von meiner familie\n= this is a picture of my family\n< this is a picture of my family <EOS>\n\n> arbeit am wochenende versuche ich zu vermeiden\n= working on weekends is something that i try to avoid doing\n< working on weekends is something that i try to <EOS>\n\n> wir beide haben das gleiche problem\n= we both have the same problem\n< both of us both those the same problem <EOS>\n\n> tom und maria haben gerade ihre verlobung bekanntgegeben\n= tom and mary just announced their engagement\n< tom and mary just announced their engagement <EOS>\n\n> tom maria und johannes spielten auf dem schulhof fangen\n= tom mary and john were playing tag on the playground\n< tom mary and john were playing tag on the <EOS>\n\n> tom muss nicht wissen wer uns das gegeben hat\n= tom doesn t need to know who gave this to us\n< tom doesn t need to know who gave this <EOS>\n\n> tom wei dass maria keine rohen eier mag\n= tom knows mary doesn t like raw eggs\n< tom knows that mary doesn t like raw eggs <EOS>\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "def showAttention(input_sentence, output_words, attentions):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n    fig.colorbar(cax)\n\n    # Set up axes\n    ax.set_xticklabels([''] + input_sentence.split(' ') +\n                       ['<EOS>'], rotation=90)\n    ax.set_yticklabels([''] + output_wordsd)\n\n    # Show label at every tick\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()\n\n\ndef evaluateAndShowAttention(input_sentence):\n    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n    print('input =', input_sentence)\n    print('output =', ' '.join(output_words))\n    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
   "metadata": {
    "id": "aQD3gOF8TKlF",
    "execution": {
     "iopub.status.busy": "2023-08-26T16:37:41.704524Z",
     "iopub.execute_input": "2023-08-26T16:37:41.704884Z",
     "iopub.status.idle": "2023-08-26T16:37:41.713900Z",
     "shell.execute_reply.started": "2023-08-26T16:37:41.704853Z",
     "shell.execute_reply": "2023-08-26T16:37:41.712725Z"
    },
    "trusted": true
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "evaluateAndShowAttention('was gerade passiert')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-26T16:37:44.882482Z",
     "iopub.execute_input": "2023-08-26T16:37:44.882833Z",
     "iopub.status.idle": "2023-08-26T16:37:45.192205Z",
     "shell.execute_reply.started": "2023-08-26T16:37:44.882803Z",
     "shell.execute_reply": "2023-08-26T16:37:45.190348Z"
    },
    "trusted": true
   },
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "text": "input = was gerade passiert\noutput = what just happened to what right now <EOS>\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/tmp/ipykernel_28/497514107.py:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mevaluateAndShowAttention\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwas gerade passiert\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[41], line 23\u001B[0m, in \u001B[0;36mevaluateAndShowAttention\u001B[0;34m(input_sentence)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput =\u001B[39m\u001B[38;5;124m'\u001B[39m, input_sentence)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput =\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(output_words))\n\u001B[0;32m---> 23\u001B[0m \u001B[43mshowAttention\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_sentence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_words\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattentions\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput_words\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[41], line 10\u001B[0m, in \u001B[0;36mshowAttention\u001B[0;34m(input_sentence, output_words, attentions)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Set up axes\u001B[39;00m\n\u001B[1;32m      8\u001B[0m ax\u001B[38;5;241m.\u001B[39mset_xticklabels([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m input_sentence\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m      9\u001B[0m                    [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<EOS>\u001B[39m\u001B[38;5;124m'\u001B[39m], rotation\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m90\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m ax\u001B[38;5;241m.\u001B[39mset_yticklabels([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[43moutput_wordsd\u001B[49m)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Show label at every tick\u001B[39;00m\n\u001B[1;32m     13\u001B[0m ax\u001B[38;5;241m.\u001B[39mxaxis\u001B[38;5;241m.\u001B[39mset_major_locator(ticker\u001B[38;5;241m.\u001B[39mMultipleLocator(\u001B[38;5;241m1\u001B[39m))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'output_wordsd' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'output_wordsd' is not defined",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "from nltk.translate.bleu_score import sentence_bleu\ndef calculate_score(encoder,decoder,pairs):\n    bleu_scores = []\n    reference = [pair[1].split(' ') for pair in pairs] \n    \n    for pair in pairs:\n        candidate, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang) \n        \n        score = sentence_bleu(reference,candidate)\n        bleu_scores.append(score)\n    \n    return (sum(bleu_scores)/len(pairs))*100",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-26T17:00:49.711111Z",
     "iopub.execute_input": "2023-08-26T17:00:49.711854Z",
     "iopub.status.idle": "2023-08-26T17:00:49.719352Z",
     "shell.execute_reply.started": "2023-08-26T17:00:49.711817Z",
     "shell.execute_reply": "2023-08-26T17:00:49.718191Z"
    },
    "trusted": true
   },
   "execution_count": 64,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print('BLEU score: {} %'.format(round(calculate_score(encoder, decoder, pairs[0:2000])), 2))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-26T17:00:50.646466Z",
     "iopub.execute_input": "2023-08-26T17:00:50.647157Z",
     "iopub.status.idle": "2023-08-26T17:02:48.340311Z",
     "shell.execute_reply.started": "2023-08-26T17:00:50.647116Z",
     "shell.execute_reply": "2023-08-26T17:02:48.339229Z"
    },
    "trusted": true
   },
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "text": "BLEU score: 59 %\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "torch.save(encoder.state_dict(), '/kaggle/working/enconder.ptk')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-26T17:15:16.642908Z",
     "iopub.execute_input": "2023-08-26T17:15:16.643291Z",
     "iopub.status.idle": "2023-08-26T17:15:16.674363Z",
     "shell.execute_reply.started": "2023-08-26T17:15:16.643258Z",
     "shell.execute_reply": "2023-08-26T17:15:16.673303Z"
    },
    "trusted": true
   },
   "execution_count": 71,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "torch.save(encoder.state_dict(), '/kaggle/working/deconder.ptk')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-26T17:15:22.088394Z",
     "iopub.execute_input": "2023-08-26T17:15:22.088747Z",
     "iopub.status.idle": "2023-08-26T17:15:22.118075Z",
     "shell.execute_reply.started": "2023-08-26T17:15:22.088717Z",
     "shell.execute_reply": "2023-08-26T17:15:22.116902Z"
    },
    "trusted": true
   },
   "execution_count": 72,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
