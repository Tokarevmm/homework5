{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "from io import open\nimport re\nimport torch\nimport time\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch import nn\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "metadata": {
    "id": "lPacUjneYeTU",
    "execution": {
     "iopub.status.busy": "2023-08-26T17:23:33.949879Z",
     "iopub.execute_input": "2023-08-26T17:23:33.951666Z",
     "iopub.status.idle": "2023-08-26T17:23:38.228803Z",
     "shell.execute_reply.started": "2023-08-26T17:23:33.951607Z",
     "shell.execute_reply": "2023-08-26T17:23:38.226701Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "LENGTH_SEQUENCE = 10\nLEARNING_RATE = 0.05\nNUM_EPOCHS = 500\nBATCH_SIZE = 512\nINPUT_DIM = 10",
   "metadata": {
    "id": "qkOOe7FTYeTY",
    "execution": {
     "iopub.status.busy": "2023-08-26T17:23:38.230757Z",
     "iopub.execute_input": "2023-08-26T17:23:38.231328Z",
     "iopub.status.idle": "2023-08-26T17:23:38.238162Z",
     "shell.execute_reply.started": "2023-08-26T17:23:38.231293Z",
     "shell.execute_reply": "2023-08-26T17:23:38.236437Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def generate_sequences(length = LENGTH_SEQUENCE):\n    x = torch.randint(0, 10, (length,))\n\n    y = torch.cat((x[0].unsqueeze(dim=0), torch.add(x[0], x[1:])))\n\n    for i in y:\n        if i >= 10:\n            i -= 10\n\n    return x.unsqueeze(dim=0), y.unsqueeze(dim=0)",
   "metadata": {
    "id": "fK23BlJ3YeTZ",
    "execution": {
     "iopub.status.busy": "2023-08-26T17:23:38.239718Z",
     "iopub.execute_input": "2023-08-26T17:23:38.240571Z",
     "iopub.status.idle": "2023-08-26T17:23:38.265927Z",
     "shell.execute_reply.started": "2023-08-26T17:23:38.240520Z",
     "shell.execute_reply": "2023-08-26T17:23:38.262951Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def create_dataset(number_sequences = 10_000, length_sequence = LENGTH_SEQUENCE, test_size = 0.2, val_size = 0.1, BATCH_SIZE = BATCH_SIZE):\n    x_train, y_train = generate_sequences(length_sequence)\n    for _ in range(number_sequences - 1):\n        x, y = generate_sequences(length_sequence)\n        x_train = torch.cat((x_train, x))\n        y_train = torch.cat((y_train, y))\n\n    x_test, y_test = generate_sequences(length_sequence)\n    for _ in range(round(number_sequences * test_size) - 1):\n        x, y = generate_sequences(length_sequence)\n        x_test = torch.cat((x_test, x))\n        y_test = torch.cat((y_test, y))\n\n    x_val, y_val = generate_sequences(length_sequence)\n    for _ in range(round(number_sequences * val_size) - 1):\n        x, y = generate_sequences(length_sequence)\n        x_val = torch.cat((x_val, x))\n        y_val = torch.cat((y_val, y))\n\n    train_dataset = TensorDataset(x_train, y_train)\n    test_dataset = TensorDataset(x_test, y_test)\n    val_dataset = TensorDataset(x_val, y_val)\n\n    train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    test = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n    return train, test, val",
   "metadata": {
    "id": "nZAafTJnYeTZ",
    "execution": {
     "iopub.status.busy": "2023-08-26T17:23:38.272584Z",
     "iopub.execute_input": "2023-08-26T17:23:38.273269Z",
     "iopub.status.idle": "2023-08-26T17:23:38.292979Z",
     "shell.execute_reply.started": "2023-08-26T17:23:38.273167Z",
     "shell.execute_reply": "2023-08-26T17:23:38.291950Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class Model(torch.nn.Module):\n\n    def __init__(self, model, embed_dim, hidden_dim, layer_dim):\n        super().__init__()\n        self.embed = nn.Embedding(INPUT_DIM, embed_dim)\n        self.model = model(embed_dim, hidden_dim, layer_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, INPUT_DIM)\n\n    def forward(self, sentence, state=None):\n        embed = self.embed(sentence)\n        o, _ = self.model(embed)\n        return self.linear(o)",
   "metadata": {
    "id": "oo4kLD8gYeTa",
    "execution": {
     "iopub.status.busy": "2023-08-26T17:23:38.294007Z",
     "iopub.execute_input": "2023-08-26T17:23:38.294389Z",
     "iopub.status.idle": "2023-08-26T17:23:38.310453Z",
     "shell.execute_reply.started": "2023-08-26T17:23:38.294357Z",
     "shell.execute_reply": "2023-08-26T17:23:38.309180Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class Training:\n    def __init__(self, model, loss_fn, optimizer):\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n\n    def train(self, train, test):\n        for epoch in range(1, NUM_EPOCHS + 1):\n            train_loss, train_accuracy, iter_num = .0, .0, .0\n            start_epoch_time = time.time()\n            self.model.train().to(device)\n            for x, y in train:\n                x = x.to(device)\n                y = y.view(1, -1).squeeze().to(device)\n\n                self.optimizer.zero_grad()\n\n                out = self.model.forward(x).view(-1, INPUT_DIM)\n\n                loss = self.loss_fn(out, y)\n                train_loss += loss.item()\n\n                batch_accuracy = (out.argmax(dim=1) == y)\n                train_accuracy += batch_accuracy.sum().item() / batch_accuracy.shape[0]\n\n                loss.backward()\n                self.optimizer.step()\n                iter_num += 1\n            if (epoch < 2) | (epoch % 10 == 0):\n                print(f\"Epoch: {epoch}, loss: {train_loss:.4f}, acc: \" f\"{train_accuracy / iter_num:.4f}\", end=\" | \")\n\n            test_loss, test_accuracy, iter_num = .0, .0, .0\n            self.model.eval().to(device)\n            for x, y in test:\n                x = x.to(device)\n                y = y.view(1, -1).squeeze().to(device)\n\n                out = self.model.forward(x).view(-1, INPUT_DIM)\n\n                loss = self.loss_fn(out, y)\n                test_loss += loss.item()\n\n                batch_accuracy = (out.argmax(dim=1) == y)\n                test_accuracy += batch_accuracy.sum().item() / batch_accuracy.shape[0]\n                iter_num += 1\n            if (epoch < 2) | (epoch % 10 == 0):\n                print(f\"test loss: {test_loss:.4f}, test acc: {test_accuracy / iter_num:.4f} | \" f\"{time.time() - start_epoch_time:.2f} sec.\")",
   "metadata": {
    "id": "yyOhkmi_YeTa",
    "execution": {
     "iopub.status.busy": "2023-08-26T17:23:38.311779Z",
     "iopub.execute_input": "2023-08-26T17:23:38.312254Z",
     "iopub.status.idle": "2023-08-26T17:23:38.334654Z",
     "shell.execute_reply.started": "2023-08-26T17:23:38.312194Z",
     "shell.execute_reply": "2023-08-26T17:23:38.332831Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train, test, val = create_dataset()",
   "metadata": {
    "id": "wXAaC-0qYeTb",
    "execution": {
     "iopub.status.busy": "2023-08-26T17:23:38.337336Z",
     "iopub.execute_input": "2023-08-26T17:23:38.337766Z",
     "iopub.status.idle": "2023-08-26T17:23:40.085419Z",
     "shell.execute_reply.started": "2023-08-26T17:23:38.337728Z",
     "shell.execute_reply": "2023-08-26T17:23:40.083796Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LSTM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": "models = {\n        \"RNN\": [nn.RNN, 32, 128, 5],\n        \"LSTM\": [nn.LSTM, 32, 64, 1],\n        \"GRU\": [nn.GRU, 32, 64, 1]\n    }\nmodel = Model(*models[\"LSTM\"])\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\ntraining = Training(model, loss_fn, optimizer)\ntraining.train(train, test)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OcZaMxKCYeTc",
    "outputId": "ad961764-5b6f-493c-af48-983cb63ce399",
    "execution": {
     "iopub.status.busy": "2023-08-26T17:23:40.087135Z",
     "iopub.execute_input": "2023-08-26T17:23:40.087991Z",
     "iopub.status.idle": "2023-08-26T17:26:59.408266Z",
     "shell.execute_reply.started": "2023-08-26T17:23:40.087958Z",
     "shell.execute_reply": "2023-08-26T17:26:59.406404Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch: 1, loss: 46.1131, acc: 0.1033 | test loss: 9.2165, test acc: 0.1038 | 0.64 sec.\nEpoch: 10, loss: 45.9358, acc: 0.1525 | test loss: 9.1825, test acc: 0.1565 | 0.38 sec.\nEpoch: 20, loss: 45.8148, acc: 0.1900 | test loss: 9.1580, test acc: 0.1953 | 0.39 sec.\nEpoch: 30, loss: 45.7207, acc: 0.1863 | test loss: 9.1389, test acc: 0.1909 | 0.40 sec.\nEpoch: 40, loss: 45.6436, acc: 0.1873 | test loss: 9.1233, test acc: 0.1930 | 0.37 sec.\nEpoch: 50, loss: 45.5826, acc: 0.1864 | test loss: 9.1105, test acc: 0.1925 | 0.43 sec.\nEpoch: 60, loss: 45.5322, acc: 0.1863 | test loss: 9.0997, test acc: 0.1923 | 0.42 sec.\nEpoch: 70, loss: 45.4903, acc: 0.1858 | test loss: 9.0911, test acc: 0.1920 | 0.37 sec.\nEpoch: 80, loss: 45.4434, acc: 0.1866 | test loss: 9.0828, test acc: 0.1920 | 0.42 sec.\nEpoch: 90, loss: 45.4000, acc: 0.1875 | test loss: 9.0751, test acc: 0.1922 | 0.39 sec.\nEpoch: 100, loss: 45.3793, acc: 0.1863 | test loss: 9.0681, test acc: 0.1922 | 0.36 sec.\nEpoch: 110, loss: 45.3451, acc: 0.1863 | test loss: 9.0625, test acc: 0.1918 | 0.39 sec.\nEpoch: 120, loss: 45.3089, acc: 0.1868 | test loss: 9.0560, test acc: 0.1918 | 0.39 sec.\nEpoch: 130, loss: 45.2840, acc: 0.1863 | test loss: 9.0495, test acc: 0.1917 | 0.37 sec.\nEpoch: 140, loss: 45.2410, acc: 0.1863 | test loss: 9.0410, test acc: 0.1919 | 0.38 sec.\nEpoch: 150, loss: 45.1974, acc: 0.1865 | test loss: 9.0318, test acc: 0.1921 | 0.35 sec.\nEpoch: 160, loss: 45.1447, acc: 0.1863 | test loss: 9.0216, test acc: 0.1919 | 0.37 sec.\nEpoch: 170, loss: 45.0810, acc: 0.1866 | test loss: 9.0088, test acc: 0.1919 | 0.38 sec.\nEpoch: 180, loss: 45.0039, acc: 0.1866 | test loss: 8.9937, test acc: 0.1916 | 0.50 sec.\nEpoch: 190, loss: 44.9032, acc: 0.1867 | test loss: 8.9723, test acc: 0.1924 | 0.37 sec.\nEpoch: 200, loss: 44.7800, acc: 0.1868 | test loss: 8.9455, test acc: 0.1928 | 0.36 sec.\nEpoch: 210, loss: 44.6018, acc: 0.1889 | test loss: 8.9104, test acc: 0.1938 | 0.41 sec.\nEpoch: 220, loss: 44.3747, acc: 0.1895 | test loss: 8.8639, test acc: 0.1938 | 0.37 sec.\nEpoch: 230, loss: 44.0840, acc: 0.1915 | test loss: 8.8033, test acc: 0.1949 | 0.39 sec.\nEpoch: 240, loss: 43.7028, acc: 0.1952 | test loss: 8.7265, test acc: 0.1985 | 0.36 sec.\nEpoch: 250, loss: 43.2401, acc: 0.2033 | test loss: 8.6311, test acc: 0.2068 | 0.33 sec.\nEpoch: 260, loss: 42.7078, acc: 0.2151 | test loss: 8.5179, test acc: 0.2195 | 0.39 sec.\nEpoch: 270, loss: 42.0579, acc: 0.2259 | test loss: 8.3889, test acc: 0.2317 | 0.41 sec.\nEpoch: 280, loss: 41.3261, acc: 0.2382 | test loss: 8.2328, test acc: 0.2438 | 0.38 sec.\nEpoch: 290, loss: 40.3262, acc: 0.2556 | test loss: 8.0284, test acc: 0.2608 | 0.41 sec.\nEpoch: 300, loss: 38.6024, acc: 0.2852 | test loss: 7.6625, test acc: 0.2910 | 0.40 sec.\nEpoch: 310, loss: 35.5236, acc: 0.3345 | test loss: 7.0373, test acc: 0.3499 | 0.37 sec.\nEpoch: 320, loss: 32.4352, acc: 0.3831 | test loss: 6.4230, test acc: 0.3927 | 0.37 sec.\nEpoch: 330, loss: 29.8768, acc: 0.4017 | test loss: 5.9244, test acc: 0.4124 | 0.38 sec.\nEpoch: 340, loss: 28.0124, acc: 0.4266 | test loss: 5.5578, test acc: 0.4362 | 0.37 sec.\nEpoch: 350, loss: 26.6861, acc: 0.4503 | test loss: 5.2938, test acc: 0.4615 | 0.37 sec.\nEpoch: 360, loss: 25.5443, acc: 0.4813 | test loss: 5.0631, test acc: 0.4918 | 0.40 sec.\nEpoch: 370, loss: 24.3202, acc: 0.5226 | test loss: 4.8220, test acc: 0.5335 | 0.42 sec.\nEpoch: 380, loss: 22.8600, acc: 0.5833 | test loss: 4.5208, test acc: 0.5955 | 0.38 sec.\nEpoch: 390, loss: 20.9200, acc: 0.6535 | test loss: 4.1264, test acc: 0.6671 | 0.38 sec.\nEpoch: 400, loss: 18.5653, acc: 0.7240 | test loss: 3.6569, test acc: 0.7350 | 0.37 sec.\nEpoch: 410, loss: 15.9776, acc: 0.7841 | test loss: 3.1499, test acc: 0.7927 | 0.36 sec.\nEpoch: 420, loss: 13.5967, acc: 0.8360 | test loss: 2.6889, test acc: 0.8418 | 0.36 sec.\nEpoch: 430, loss: 11.5429, acc: 0.8835 | test loss: 2.2856, test acc: 0.8854 | 0.36 sec.\nEpoch: 440, loss: 9.6890, acc: 0.9273 | test loss: 1.9171, test acc: 0.9302 | 0.38 sec.\nEpoch: 450, loss: 7.9932, acc: 0.9607 | test loss: 1.5746, test acc: 0.9621 | 0.41 sec.\nEpoch: 460, loss: 6.4676, acc: 0.9823 | test loss: 1.2734, test acc: 0.9830 | 0.38 sec.\nEpoch: 470, loss: 5.2345, acc: 0.9933 | test loss: 1.0299, test acc: 0.9931 | 0.44 sec.\nEpoch: 480, loss: 4.2577, acc: 0.9974 | test loss: 0.8403, test acc: 0.9969 | 0.39 sec.\nEpoch: 490, loss: 3.5148, acc: 0.9985 | test loss: 0.6941, test acc: 0.9984 | 0.37 sec.\nEpoch: 500, loss: 2.9485, acc: 0.9997 | test loss: 0.5832, test acc: 0.9996 | 0.41 sec.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": "model = Model(*models[\"RNN\"])\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\ntraining = Training(model, loss_fn, optimizer)\ntraining.train(train, test)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-26T17:56:02.344923Z",
     "iopub.execute_input": "2023-08-26T17:56:02.345589Z",
     "iopub.status.idle": "2023-08-26T18:05:50.217870Z",
     "shell.execute_reply.started": "2023-08-26T17:56:02.345533Z",
     "shell.execute_reply": "2023-08-26T18:05:50.216029Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch: 1, loss: 46.1116, acc: 0.1002 | test loss: 9.2159, test acc: 0.0982 | 1.67 sec.\nEpoch: 10, loss: 45.9807, acc: 0.1441 | test loss: 9.1943, test acc: 0.1479 | 1.53 sec.\nEpoch: 20, loss: 45.8853, acc: 0.1651 | test loss: 9.1744, test acc: 0.1661 | 1.44 sec.\nEpoch: 30, loss: 45.7732, acc: 0.1770 | test loss: 9.1502, test acc: 0.1834 | 1.54 sec.\nEpoch: 40, loss: 45.6413, acc: 0.1800 | test loss: 9.1237, test acc: 0.1844 | 1.44 sec.\nEpoch: 50, loss: 45.5102, acc: 0.1876 | test loss: 9.0965, test acc: 0.1920 | 1.35 sec.\nEpoch: 60, loss: 45.3080, acc: 0.1865 | test loss: 9.0552, test acc: 0.1916 | 1.77 sec.\nEpoch: 70, loss: 44.6807, acc: 0.1861 | test loss: 8.9179, test acc: 0.1911 | 1.44 sec.\nEpoch: 80, loss: 43.1533, acc: 0.1877 | test loss: 8.5994, test acc: 0.1906 | 1.45 sec.\nEpoch: 90, loss: 41.5113, acc: 0.1974 | test loss: 8.2725, test acc: 0.2078 | 1.40 sec.\nEpoch: 100, loss: 40.3291, acc: 0.2088 | test loss: 8.0385, test acc: 0.2217 | 1.85 sec.\nEpoch: 110, loss: 39.1934, acc: 0.2216 | test loss: 7.7928, test acc: 0.2369 | 1.61 sec.\nEpoch: 120, loss: 38.0844, acc: 0.2425 | test loss: 7.5750, test acc: 0.2556 | 2.01 sec.\nEpoch: 130, loss: 37.8198, acc: 0.2466 | test loss: 7.5077, test acc: 0.2596 | 1.58 sec.\nEpoch: 140, loss: 37.6420, acc: 0.2528 | test loss: 7.4839, test acc: 0.2518 | 1.53 sec.\nEpoch: 150, loss: 37.5439, acc: 0.2538 | test loss: 7.4669, test acc: 0.2572 | 1.80 sec.\nEpoch: 160, loss: 37.4711, acc: 0.2556 | test loss: 7.4493, test acc: 0.2594 | 1.60 sec.\nEpoch: 170, loss: 37.3377, acc: 0.2568 | test loss: 7.4278, test acc: 0.2608 | 1.47 sec.\nEpoch: 180, loss: 37.2421, acc: 0.2599 | test loss: 7.4035, test acc: 0.2661 | 1.50 sec.\nEpoch: 190, loss: 37.0888, acc: 0.2606 | test loss: 7.3774, test acc: 0.2637 | 1.81 sec.\nEpoch: 200, loss: 36.8994, acc: 0.2668 | test loss: 7.3429, test acc: 0.2696 | 1.67 sec.\nEpoch: 210, loss: 36.7035, acc: 0.2763 | test loss: 7.3049, test acc: 0.2797 | 1.67 sec.\nEpoch: 220, loss: 36.4214, acc: 0.2901 | test loss: 7.2546, test acc: 0.2854 | 1.47 sec.\nEpoch: 230, loss: 35.8574, acc: 0.3091 | test loss: 7.2344, test acc: 0.2955 | 1.54 sec.\nEpoch: 240, loss: 33.8998, acc: 0.3690 | test loss: 6.7311, test acc: 0.3834 | 1.48 sec.\nEpoch: 250, loss: 32.1222, acc: 0.4045 | test loss: 6.4110, test acc: 0.4045 | 1.52 sec.\nEpoch: 260, loss: 30.9460, acc: 0.4295 | test loss: 6.1348, test acc: 0.4319 | 1.73 sec.\nEpoch: 270, loss: 22.8418, acc: 0.5136 | test loss: 4.5466, test acc: 0.5027 | 1.62 sec.\nEpoch: 280, loss: 20.3390, acc: 0.5381 | test loss: 4.0888, test acc: 0.5344 | 1.52 sec.\nEpoch: 290, loss: 18.7036, acc: 0.6088 | test loss: 3.5856, test acc: 0.6217 | 1.37 sec.\nEpoch: 300, loss: 10.0218, acc: 0.8231 | test loss: 1.7212, test acc: 0.8596 | 1.56 sec.\nEpoch: 310, loss: 2.8628, acc: 0.9775 | test loss: 0.5258, test acc: 0.9827 | 1.53 sec.\nEpoch: 320, loss: 0.7494, acc: 1.0000 | test loss: 0.1461, test acc: 0.9999 | 1.49 sec.\nEpoch: 330, loss: 0.4188, acc: 1.0000 | test loss: 0.0828, test acc: 1.0000 | 1.55 sec.\nEpoch: 340, loss: 0.2866, acc: 1.0000 | test loss: 0.0572, test acc: 1.0000 | 1.63 sec.\nEpoch: 350, loss: 0.2166, acc: 1.0000 | test loss: 0.0433, test acc: 1.0000 | 1.57 sec.\nEpoch: 360, loss: 0.1731, acc: 1.0000 | test loss: 0.0347, test acc: 1.0000 | 1.73 sec.\nEpoch: 370, loss: 0.1436, acc: 1.0000 | test loss: 0.0289, test acc: 1.0000 | 1.58 sec.\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mLEARNING_RATE)\n\u001B[1;32m      4\u001B[0m training \u001B[38;5;241m=\u001B[39m Training(model, loss_fn, optimizer)\n\u001B[0;32m----> 5\u001B[0m \u001B[43mtraining\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 26\u001B[0m, in \u001B[0;36mTraining.train\u001B[0;34m(self, train, test)\u001B[0m\n\u001B[1;32m     23\u001B[0m batch_accuracy \u001B[38;5;241m=\u001B[39m (out\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m==\u001B[39m y)\n\u001B[1;32m     24\u001B[0m train_accuracy \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m batch_accuracy\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m/\u001B[39m batch_accuracy\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m---> 26\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     28\u001B[0m iter_num \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GRU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": "model = Model(*models[\"GRU\"])\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\ntraining = Training(model, loss_fn, optimizer)\ntraining.train(train, test)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-26T18:05:55.363544Z",
     "iopub.execute_input": "2023-08-26T18:05:55.363954Z",
     "iopub.status.idle": "2023-08-26T18:10:34.770182Z",
     "shell.execute_reply.started": "2023-08-26T18:05:55.363920Z",
     "shell.execute_reply": "2023-08-26T18:10:34.769349Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch: 1, loss: 46.1799, acc: 0.1099 | test loss: 9.2206, test acc: 0.1086 | 0.58 sec.\nEpoch: 10, loss: 45.6938, acc: 0.1840 | test loss: 9.1323, test acc: 0.1885 | 0.59 sec.\nEpoch: 20, loss: 45.5443, acc: 0.1862 | test loss: 9.1007, test acc: 0.1921 | 0.50 sec.\nEpoch: 30, loss: 45.4809, acc: 0.1857 | test loss: 9.0870, test acc: 0.1919 | 0.57 sec.\nEpoch: 40, loss: 45.4240, acc: 0.1867 | test loss: 9.0777, test acc: 0.1918 | 0.49 sec.\nEpoch: 50, loss: 45.3918, acc: 0.1861 | test loss: 9.0701, test acc: 0.1917 | 0.68 sec.\nEpoch: 60, loss: 45.3481, acc: 0.1864 | test loss: 9.0617, test acc: 0.1920 | 0.49 sec.\nEpoch: 70, loss: 45.3105, acc: 0.1862 | test loss: 9.0533, test acc: 0.1919 | 0.53 sec.\nEpoch: 80, loss: 45.2502, acc: 0.1869 | test loss: 9.0438, test acc: 0.1920 | 0.51 sec.\nEpoch: 90, loss: 45.1976, acc: 0.1865 | test loss: 9.0313, test acc: 0.1923 | 0.49 sec.\nEpoch: 100, loss: 45.1216, acc: 0.1868 | test loss: 9.0175, test acc: 0.1920 | 0.47 sec.\nEpoch: 110, loss: 45.0297, acc: 0.1870 | test loss: 8.9993, test acc: 0.1919 | 0.47 sec.\nEpoch: 120, loss: 44.9157, acc: 0.1872 | test loss: 8.9768, test acc: 0.1917 | 0.54 sec.\nEpoch: 130, loss: 44.7777, acc: 0.1866 | test loss: 8.9459, test acc: 0.1921 | 0.59 sec.\nEpoch: 140, loss: 44.5881, acc: 0.1868 | test loss: 8.9076, test acc: 0.1923 | 0.61 sec.\nEpoch: 150, loss: 44.3483, acc: 0.1891 | test loss: 8.8607, test acc: 0.1931 | 0.50 sec.\nEpoch: 160, loss: 44.0710, acc: 0.1920 | test loss: 8.8028, test acc: 0.1982 | 0.52 sec.\nEpoch: 170, loss: 43.7560, acc: 0.1938 | test loss: 8.7376, test acc: 0.1995 | 0.48 sec.\nEpoch: 180, loss: 43.3866, acc: 0.1986 | test loss: 8.6644, test acc: 0.2022 | 0.59 sec.\nEpoch: 190, loss: 43.0091, acc: 0.2026 | test loss: 8.5878, test acc: 0.2056 | 0.46 sec.\nEpoch: 200, loss: 42.6259, acc: 0.2077 | test loss: 8.5118, test acc: 0.2108 | 0.59 sec.\nEpoch: 210, loss: 42.2535, acc: 0.2138 | test loss: 8.4334, test acc: 0.2151 | 0.54 sec.\nEpoch: 220, loss: 41.8609, acc: 0.2193 | test loss: 8.3590, test acc: 0.2198 | 0.52 sec.\nEpoch: 230, loss: 41.5074, acc: 0.2225 | test loss: 8.2868, test acc: 0.2228 | 0.58 sec.\nEpoch: 240, loss: 41.1211, acc: 0.2283 | test loss: 8.2120, test acc: 0.2275 | 0.56 sec.\nEpoch: 250, loss: 40.7295, acc: 0.2336 | test loss: 8.1341, test acc: 0.2340 | 0.51 sec.\nEpoch: 260, loss: 40.2913, acc: 0.2412 | test loss: 8.0469, test acc: 0.2406 | 0.68 sec.\nEpoch: 270, loss: 39.7469, acc: 0.2548 | test loss: 7.9426, test acc: 0.2511 | 0.50 sec.\nEpoch: 280, loss: 38.9109, acc: 0.2763 | test loss: 7.7763, test acc: 0.2749 | 0.62 sec.\nEpoch: 290, loss: 37.1829, acc: 0.3160 | test loss: 7.4239, test acc: 0.3158 | 0.76 sec.\nEpoch: 300, loss: 34.8200, acc: 0.3779 | test loss: 6.9585, test acc: 0.3798 | 0.56 sec.\nEpoch: 310, loss: 32.3107, acc: 0.4430 | test loss: 6.4531, test acc: 0.4433 | 0.56 sec.\nEpoch: 320, loss: 29.3874, acc: 0.5125 | test loss: 5.8715, test acc: 0.5119 | 0.64 sec.\nEpoch: 330, loss: 26.1592, acc: 0.5789 | test loss: 5.2137, test acc: 0.5810 | 0.50 sec.\nEpoch: 340, loss: 21.8967, acc: 0.6734 | test loss: 4.3504, test acc: 0.6751 | 0.54 sec.\nEpoch: 350, loss: 18.3333, acc: 0.7638 | test loss: 3.6389, test acc: 0.7631 | 0.73 sec.\nEpoch: 360, loss: 14.6931, acc: 0.8724 | test loss: 2.9219, test acc: 0.8700 | 0.53 sec.\nEpoch: 370, loss: 11.3091, acc: 0.9535 | test loss: 2.2444, test acc: 0.9511 | 0.52 sec.\nEpoch: 380, loss: 8.4258, acc: 0.9899 | test loss: 1.6719, test acc: 0.9910 | 0.57 sec.\nEpoch: 390, loss: 6.3076, acc: 0.9982 | test loss: 1.2531, test acc: 0.9984 | 0.52 sec.\nEpoch: 400, loss: 4.8346, acc: 0.9998 | test loss: 0.9634, test acc: 0.9997 | 0.57 sec.\nEpoch: 410, loss: 3.8053, acc: 1.0000 | test loss: 0.7594, test acc: 1.0000 | 0.54 sec.\nEpoch: 420, loss: 3.0589, acc: 1.0000 | test loss: 0.6116, test acc: 1.0000 | 0.52 sec.\nEpoch: 430, loss: 2.5074, acc: 1.0000 | test loss: 0.5011, test acc: 1.0000 | 0.60 sec.\nEpoch: 440, loss: 2.0922, acc: 1.0000 | test loss: 0.4184, test acc: 1.0000 | 0.92 sec.\nEpoch: 450, loss: 1.7754, acc: 1.0000 | test loss: 0.3554, test acc: 1.0000 | 0.52 sec.\nEpoch: 460, loss: 1.5320, acc: 1.0000 | test loss: 0.3063, test acc: 1.0000 | 0.48 sec.\nEpoch: 470, loss: 1.3398, acc: 1.0000 | test loss: 0.2679, test acc: 1.0000 | 0.52 sec.\nEpoch: 480, loss: 1.1866, acc: 1.0000 | test loss: 0.2371, test acc: 1.0000 | 0.52 sec.\nEpoch: 490, loss: 1.0607, acc: 1.0000 | test loss: 0.2121, test acc: 1.0000 | 0.51 sec.\nEpoch: 500, loss: 0.9574, acc: 1.0000 | test loss: 0.1914, test acc: 1.0000 | 0.50 sec.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "x, y = generate_sequences()\n\nx = x.to(device)\ny = y.to(device)\nout = model.forward(x).view(-1, INPUT_DIM).argmax(dim=1)\n\nprint(f\"Original sentence is : {x}\")\nprint(\"-\" * 100)\nprint(f\"Encrypted sentence is : {y}\")\nprint(\"-\" * 100)\nprint(f\"Predicted sentence is : {out}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHdkgRhbYeTd",
    "outputId": "7b45b812-59cc-40e9-851b-31390ae57d6f",
    "execution": {
     "iopub.status.busy": "2023-08-26T18:10:40.845127Z",
     "iopub.execute_input": "2023-08-26T18:10:40.845573Z",
     "iopub.status.idle": "2023-08-26T18:10:40.862264Z",
     "shell.execute_reply.started": "2023-08-26T18:10:40.845542Z",
     "shell.execute_reply": "2023-08-26T18:10:40.861007Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": "Original sentence is : tensor([[4, 0, 6, 0, 9, 5, 8, 6, 5, 3]])\n----------------------------------------------------------------------------------------------------\nEncrypted sentence is : tensor([[4, 4, 0, 4, 3, 9, 2, 0, 9, 7]])\n----------------------------------------------------------------------------------------------------\nPredicted sentence is : tensor([4, 4, 0, 4, 3, 9, 2, 0, 9, 7])\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "PlMFW0p6bEIN"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
